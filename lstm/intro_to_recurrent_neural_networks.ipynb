{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Deep Learning Applications with Keras: Recurrent Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are talking about Deep Learning approaches for handling task data, one of the most common approaches is to use a Recurrent Neural Network (RNN). This networks act not only on their current input, but they have a memory composed on all the inputs it has already seen, as can be seen on the following image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img align=\"middle\" width='600' heith='100' src=\"images/lstm.png\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods read text in a similar way as humans do. When we read one word in a phrase, we know the meaning of this word based on the words we have already read. Therefore, we are able to identify the context that the word is appearing. It is expected that a Recurrent Neural Network captures the same context information when handling text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see in the above image that each word is converted into an **embedding** representation. An embedding is a vectorial representation of a word. It is used to map a word into a low dimensional space and to also capture semantic similarities between words. If you want to read more about word embeddings, please take a look at this [post](mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all that said, let's implement our first Recurrent Neural Network in Keras. We will be using these network to deal with a sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Recurrent Neural Networks in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is an important task in the Natural Language Processing field. It can be used to understand how users are responding to some new technology, marketing campaign, political idea and any kind of application which users approval is highly correlated with success. Therefore, it is important to have models to accurately handle this kind of tasks.\n",
    "\n",
    "Although when dealing with sentiment analysis, we can, for example, count the number of \"positive\" and \"negative\" words in a text and use that to judge the text polarity, there are some times that simple approaches like these one are not going to work. \n",
    "\n",
    "This happens a lot when we have a long text or text where the sentiment is captures not by single words appearing in it, but in the context that these words appear. One example, is movie reviews such as this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I happened across \"Bait\" on cable one night just as it started and thought, \"Eh, why not?\" I'm glad I gave it a chance. \"Bait\" ain't perfect. It suffers from unnecessarily flashy direction and occasional dumbness. But overall, this movie worked. All the elements aligned just right, and they pulled off what otherwise could have been a pretty ugly film. Most of that, I think, is due to Jamie Foxx. I don't know who tagged Foxx for the lead, but whoever it was did this movie a big favor. Believable and amazingly likeable, Foxx glides through the movie, smooth as butter and funnier than          hell. You can tell he's working on instinct, and instinct doesn't fail him. The plot, while unimportant, actually ties together pretty well, and there's even a character arc through which Foxx's character grows as a person. Again, they could've slipped by without any of this, but it just makes things that much better. I'm surprised at the low rating for this. Maybe I just caught this move on the right night, or vice versa, but I'd give it a 7/10. Bravo, Mssr. Foxx.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This a movie review with a positive polarity. We can see that this movie review has both positive and negative words, such as **dumbness**, **ugly**, **unimportant**, **hell**, **funnier**, and other words. Also we can see that the text has combination of words that could not be analyzied alone, for example **pretty ugly**, which is an oxymoron. **Pretty** and **ugly** have opposite meanings, but in that sentence, we see that **pretty** is being used to but emphasis on **ugly**. Another similar expression here is **ain't perfect**, where **ain't** act as a negation to the word **perfect** that comes right after this word.\n",
    "\n",
    "Therefore, we can see that to analyze such a text, we need more than counting ocurrences of words. We need to understand the context that these words appear in. That's why we will be using a Recurrent Neural Network for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that said, the task we will solve is to analyse sentiment in movie reviews similar to the one we have analyzed above. We will use th **IMDB dataset**, which contains 25000 reviews for training and 25000 reviews for testing the model. This dataset is also perfectly balances, ,meaning that we have exactly 125000 reviews with positive sentiment and 12500 reviews with negative sentiment in the training data. The dataset is also perfectly balanced on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img align=\"middle\" width='600' heith='100' src=\"images/imdb.png\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load this dataset in memory and take a closer look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "\"\"\"\n",
    "The size of our vocabulary or the number of unique words that we will use to solve our problem.\n",
    "\"\"\"\n",
    "num_words = 20000\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "print('Number of train movie reviews: {}'.format(len(x_train)))\n",
    "print('Number of test movie reviews: {}'.format(len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at an example of movie review in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Example of a movie review:\\n{}\\n'.format(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a minute, we are going to classify movie reviews, so where are the words ?? And are those numbers ??\n",
    "\n",
    "Remember our **num_words** variable ? This variable dictates how many words are going to be used to solve our problem. What this means is that we have a list of valid words that we can use, for example, our list could be something like:\n",
    "\n",
    "valid_words = [good, bad, the, is, great, terrible]\n",
    "\n",
    "We can see that each of these words have a position in that list, for example, the word good has position 1, the word terrible has position 6. These positions are the number we see in the above example. Every word in our movie review has been mapped to the position it has in the valid words list.\n",
    "\n",
    "But now you may be wondering, why would we replace the words by their position in the list ? Well, remember from the Recurrent Neural Network image in the beggining of our document that we must convert our word into **embedding**. An embedding is a vector representation of our word, for example, we can convert the word **good** into:\n",
    "\n",
    "good = [0.3, 0.2, 0.6, -0.61, 012]\n",
    "\n",
    "Every word in our valid_words list has an associated vector for it. Therefore, we have an embedding matrix, with dimensions V X D, where V is the size of our valid words list and D is the dimension of our embedding vector. This dimension represents the amount of infomation we are keeping for a word. With more dimensions, more information we can store about a word, for example, that it is similar to the words **great** and **terrific** or that it has positive sentiment. However, a bigger dimension also means that he have a model that is more slow to train and a model with more flexibility, meaning that can overfit more easily to a task.\n",
    "\n",
    "With all that said, the numbers is the list are the positions of the words in valid_words list and these numbers are used to select a row in the embedding matrix, allowing us to turn a word into a **embedding** representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we train our model, we need to perform one last step. To understand what this step is, take a look at the size of three consecutive reviews in our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print([len(review) for review in x_train[:3]], sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each movie review has a different size. This makes a lot of sense, since different reviews should not have the same number of words. However, to train our model with more than one movie review at the same time, we cannot allow movie reviews to have different sizes. To solve this problem, we will pad our movie revies.\n",
    "\n",
    "But what do I mean by padding ? Imagine that we have the following movie reviews:\n",
    "\n",
    "98, 52, 58, 12, 15\n",
    "\n",
    "86, 52, 10\n",
    "\n",
    "90, 72, 36, 78, 92, 42\n",
    "\n",
    "Remember that each of these numbers is the word position in the valid_words list. Now, when we pad our sentences, this is what happens:\n",
    "\n",
    "00, 98, 52, 58, 12, 15\n",
    "\n",
    "00, 00, 00, 86, 52, 10\n",
    "\n",
    "90, 72, 36, 78, 92, 42\n",
    "\n",
    "We can see that we have added the **00** number before each movie review, allowing the reviews to have the same size. This **00** token is the **pad token** and it carries no meaning. Therefore, our model will ignore these tokens when we see than.\n",
    "\n",
    "Now to pad our reviews, we can do the following step in keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "\"\"\"\n",
    "maxlen refers to the maximum size of a movie review in our dataset. This means that reviews with more\n",
    "than 80 words in it will be cutted to 100 words. Also, reviews with fewer words than 100 will be padded\n",
    "with our pad tokens\n",
    "\"\"\"\n",
    "maxlen = 100\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the size of our three consecutive movie reviews again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print([len(review) for review in x_train[:3]], sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that the reviews have the same size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finaly, we can implement our model. Our model will consist of three keras layers, an Embedding layer, to map each word\n",
    "into an embedding array. A [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) layer, which is a Recurrent Neural Network that better handle long dependecies between words, and a Dense layer with the sigmoid activation, to turn the output of our Recurrent Neural Network into a binary probability of a review being positive or negative.\n",
    "\n",
    "First, let's define a class to hold the model hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecurrentConfig:\n",
    "\n",
    "    def __init__(self,\n",
    "                 batch_size=32,\n",
    "                 embedding_size=50,\n",
    "                 num_words=20000,\n",
    "                 lstm_units=64,\n",
    "                 num_classes=1,\n",
    "                 epochs=5):\n",
    "\n",
    "        \"\"\"\n",
    "        Holds Recurrent Neural Network model hyperparams.\n",
    "\n",
    "        :param batch_size: batch size for training\n",
    "        :type batch_size: int\n",
    "        \n",
    "        :param embedding_size: The dimension of our embedding.\n",
    "                               Recall that the embedding is a V X D matrix, where V is the number of\n",
    "                               words in the vocabulary and D is the dimension of the embeddings.\n",
    "                               This variable represents the D value in the embedding matrix.\n",
    "        :type embedding_size:  int\n",
    "        \n",
    "        :param num_words: The size of our vocabulary or the number of words in the word_list variable.\n",
    "        :type num_words:  int\n",
    "        \n",
    "        :param lstm_units: The number of units in the LSTM layer.\n",
    "        :type num_words:  int\n",
    "    \n",
    "        :param num_classes: number of classes in the problem\n",
    "        :type epochs: int\n",
    "        \n",
    "        :param epochs: number of epochs\n",
    "        :type epochs: int\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_words = num_words\n",
    "        self.lstm_units=lstm_units\n",
    "        self.num_classes = num_classes\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def __str__(self):\n",
    "        status = ''\n",
    "        status += 'batch size: {}\\n'.format(self.batch_size)\n",
    "        status += 'embedding size: {}\\n'.format(self.embedding_size)\n",
    "        status += 'num words: {}\\n'.format(self.num_words)\n",
    "        status += 'lstm units: {}\\n'.format(self.lstm_units)\n",
    "        status += 'num classes: {}\\n'.format(self.num_classes)\n",
    "        status += 'epochs: {}\\n'.format(self.epochs)\n",
    "\n",
    "        return status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_config = RecurrentConfig()\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the config class defined, we can create our model by writting our code in the **build_graph** method bellow. Here we will define our model and build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "\n",
    "from model.model import Model\n",
    "\n",
    "\n",
    "class RecurrentModel(Model):\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        print(self.model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see our model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RecurrentModel(model_config)\n",
    "model.build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are supposed to see three layers in the model, an Embedding, a LSTM and a Dense layer. If your model has all of this layers, it is time to see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = (x_train, y_train)\n",
    "model_history = model.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate our model in the test set and understand if our model is generalizing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = (x_test, y_test)\n",
    "loss, accuracy = model.evaluate(test_data)\n",
    "\n",
    "print('Test loss: {:.2f}'.format(loss))\n",
    "print('Test accuracy: {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is expected that our model has a similar accuracy on the test set than the one achieved on the train dataset. It is expected that we should achieve around 89% accuracy on the training data and 80% on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the test accuracy is a good indicator of how our model is performing, there is an important plot we must also observe to undertand how our model is performing, it is named \"the train vs validation\" graph. In this graph, we plot for each epoch the train loss and the validation loss.\n",
    "\n",
    "For our model, we can see this graph below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train_validation_graph(train_values, validation_values, title, ylabel, legend_loc='lower right'):\n",
    "    plt.plot(train_values)\n",
    "    plt.plot(validation_values)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc=legend_loc)\n",
    "    \n",
    "plot_train_validation_graph(\n",
    "    train_values=model_history.history['loss'],\n",
    "    validation_values=model_history.history['val_loss'],\n",
    "    title='Train loss vs Validation loss',\n",
    "    ylabel='loss',\n",
    "    legend_loc='upper right'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **model_history** object holds information of variables collected during the training step of our model. This means that we can also plot the accuracy comparisons between the train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_train_validation_graph(\n",
    "    train_values=model_history.history['acc'],\n",
    "    validation_values=model_history.history['val_acc'],\n",
    "    title='Train Accuracy vs Validation Accuracy',\n",
    "    ylabel='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model loss and train accuracy start to diverge from each other. This graph can shows us some troubles in our model. For example, imagine that you plot your graph and it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_accuracy = [0.3, 0.4, 0.4, 0.45, 0.48]\n",
    "val_accuracy = [0.2, 0.3, 0.3, 0.4, 0.4]\n",
    "\n",
    "plot_train_validation_graph(\n",
    "    train_values=train_accuracy,\n",
    "    validation_values=val_accuracy,\n",
    "    title='Train Accuracy vs Validation Accuracy',\n",
    "    ylabel='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph, we can see that neither our train or validation set achieves high accuracy on the sets. This can be an indicator that our model is suffering from **underfitting**. This means that our model is not robust enough to handle our current data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that our graph is showing this type of behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_accuracy = [0.3, 0.6, 0.8, 0.95, 1.0]\n",
    "val_accuracy = [0.2, 0.3, 0.3, 0.4, 0.4]\n",
    "\n",
    "plot_train_validation_graph(\n",
    "    train_values=train_accuracy,\n",
    "    validation_values=val_accuracy,\n",
    "    title='Train Accuracy vs Validation Accuracy',\n",
    "    ylabel='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that our model is achieving a perfect accuracy in the training set, meaning that it can correctly predict the entire training set. However, this is not happening for our validation set, where the accuracy is quite low. In that case, our model is suffering of **overfitting**, meaning that the model is too robust for our dataset. In that case the model is capturing noise in our data, but it's not generalizing well to unseen data, which is a huge problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both **underfitting** and **overfitting** there are some strategies we can use to solve this type of problem:\n",
    "    \n",
    "**Underfitting**\n",
    " \n",
    "* Use a more complex model: Maybe a more robust model will be better on the task. Try adding more layers or increasing the  existent number of units in the model's layers.\n",
    "* Add more variables to the data: Maybe the features we are using are not enough to fit the data well. Therefore we should add more features to the dataset.\n",
    "\n",
    "**Overfitting**\n",
    "\n",
    "* Try using a simpler model: Maybe your model is too robust for the task. Try removing some layers from your model or reducing the number of units in the model's layers.\n",
    "* Add regularization strategies: Add regularization techniques to your model such as [dropout](https://keras.io/layers/core/#dropout) or L2 regularization on the model weights. To learn more about regularizers, take a look at this [post](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)\n",
    "* Collect more data: If even reducing the model and adding regularizers to your model doesn't work, maybe the best approach is too collect more data. Actually, this is almost always the approach which will yield the best results, but it can also be costly. If that is the case, try the other two approaches before this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all that said, let's now move to a more advanced problem using Recurrent Neural Networks on the part two of this notebook :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
