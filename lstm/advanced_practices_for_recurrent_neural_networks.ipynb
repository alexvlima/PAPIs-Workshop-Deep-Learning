{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Deep Learning Applications with Keras: Advanced Practices for Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to do sentiment analysis in a more real environment. We will be performming sentiment analysis on the tweets referencing the first debate of the republican party (GOP). Sentiment analysis for this type of political event can be important to understand who is reacting negatively to the debate. Maybe the people who are reacting with negative sentiment are people who support the democratic party or maybe these are people that are republican supporters. But in order to answer these questions, we must first understand the sentiment polarity of the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset consists of 13871 tweets and now we have three disctint labels for the tweets: **Positive**, **Negative** and **Neutral** tweets.\n",
    "\n",
    "However, this dataset is highly imbalanced, as can be seen in the following image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tweet_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following number for each tweets in our dataset:\n",
    "    \n",
    "* **Negative**: 8493\n",
    "* **Neutral**:  3142\n",
    "* **Positive**: 2236\n",
    "\n",
    "This is much more realistic setting than the imdb dataset we have used. Most of the time, when we collect a dataset, it will be imbalanced, and we will need to handle such an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in mind, I have split this dataset into a training and a test dataset. Let's load the training dataset and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data_path = 'data/gop_tweets/train.csv'\n",
    "train_data = pd.read_csv(train_data_path, index_col=0)\n",
    "\n",
    "test_data_path = 'data/gop_tweets/test.csv'\n",
    "test_data = pd.read_csv(test_data_path, index_col=0)\n",
    "\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn any pandas columm into an array by using the `.values`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have two variables, **text** that holds the tweet, and **sentiment**, that holds the polarity of the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we use what we learned about Recurrent models, we need to transform the data into a format that the model can understand. We will need to do the following steps:\n",
    "\n",
    "* **Pre-process the tweets**: In this step, we will clean the text data. Maybe we should turn all the words into lowercase or maybe there are some characters that will not help us much in this task, like punctuation.\n",
    "* **Tokenization**: In this step we choose how many words we are going to allow in our vocabulary and create our valid_words list. After that, we must convert each of the words into the position they have in the valid_words list.\n",
    "* **Padding**: We must make every tweet to have the same size by padding smaller tweets.\n",
    "* **Labels to One-Hot**: We must turn each of our labels into an one-hot encoding representation.\n",
    "\n",
    "We will apply these steps for both training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that will receive a pandas DataFrame and apply text pre-processing in it. Here is an example of a pre-prossing function. We are removing retweets tag \"RT\", lowering case every word and removing punctuation. Feel free to update this function as you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def pre_process_text(data):\n",
    "    data['text'] = data['text'].apply((lambda x: re.sub('RT','',x)))\n",
    "    data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "    data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply tokenization. Here we will define how many words we want to use for this problem and turn each word into a position id.\n",
    "\n",
    "Create a function that receives a pandas DataFrame and turn its text column into a list of positions ids.\n",
    "\n",
    "**Hint**: Keras has a class called **Tokenizer** that will be really helpfull here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def tokenize_texts(data, num_words):\n",
    "    ###YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply padding to the sentece tokens we have created.\n",
    "\n",
    "Create a function that will receive the list of reviews turned into tokens and pad them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "def pad_reviews(review_tokens, maxlen):\n",
    "    ###YOUR CODE HERE\n",
    "\n",
    "    ###END YOUR CODE\n",
    "    \n",
    "    return pad_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's turn the **sentiment** column into a list of one-hot encoding. For example, given that we have a **Neutral** sentiment for a review, its representation as an one-hot enconding vector will be:\n",
    "\n",
    "[0, 1, 0]\n",
    "\n",
    "If it was **Positive**, it will be:\n",
    "\n",
    "[0, 0, 1]\n",
    "\n",
    "This means that an one-hot encoding will create a vector with size equal to the number of labels for the problem and place a **1** only in the position associated with the given label. Here, we have set the position of the labels based on\n",
    "the alphabetic order of the name of the labels.\n",
    "\n",
    "Create a function that will receive a pandas DataFrame and return an array with the one-hot encoding of **sentiment** column.\n",
    "\n",
    "**Hint**: We can use the **get_dummies** to turn the labels into one hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_one_hot(train_data):\n",
    "    ###YOUR CODE HERE\n",
    "\n",
    "    ###END YOUR CODE\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pre_process_text(train_data)\n",
    "test_data = pre_process_text(test_data)\n",
    "\n",
    "#HERE YOU WILL DEFINE THE VALUE OF THIS VARIABLE\n",
    "num_words = 2000\n",
    "x_train = tokenize_texts(train_data, num_words)\n",
    "x_test = tokenize_texts(test_data, num_words)\n",
    "\n",
    "#HERE YOU WILL DEFINE THE VALUE OF THIS VARIABLE\n",
    "maxlen = 20\n",
    "x_train = pad_reviews(x_train, maxlen)\n",
    "x_test = pad_reviews(x_test, maxlen)\n",
    "\n",
    "y_train = label_to_one_hot(train_data)\n",
    "y_test = label_to_one_hot(test_data)\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create our model. Again we will use the same class for config and the Recurrent Model. However, feel free to add and change any option on the config class and Update the model as you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentConfig:\n",
    "\n",
    "    def __init__(self,\n",
    "                 batch_size=32,\n",
    "                 embedding_size=50,\n",
    "                 num_words=2000,\n",
    "                 lstm_units=64,\n",
    "                 num_classes=3,\n",
    "                 epochs=5):\n",
    "\n",
    "        \"\"\"\n",
    "        Holds Recurrent Neural Network model hyperparams.\n",
    "\n",
    "        :param batch_size: batch size for training\n",
    "        :type batch_size: int\n",
    "        \n",
    "        :param embedding_size: The dimension of our embedding.\n",
    "                               Recall that the embedding is a V X D matrix, where V is the number of\n",
    "                               words in the vocabulary and D is the dimension of the embeddings.\n",
    "                               This variable represents the D value in the embedding matrix.\n",
    "        :type embedding_size:  int\n",
    "        \n",
    "        :param num_words: The size of our vocabulary or the number of words in the word_list variable.\n",
    "        :type num_words:  int\n",
    "        \n",
    "        :param lstm_units: The number of units in the LSTM layer.\n",
    "        :type num_words:  int\n",
    "    \n",
    "        :param num_classes: number of classes in the problem\n",
    "        :type epochs: int\n",
    "        \n",
    "        :param epochs: number of epochs\n",
    "        :type epochs: int\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_words = num_words\n",
    "        self.lstm_units=lstm_units\n",
    "        self.num_classes = num_classes\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def __str__(self):\n",
    "        status = ''\n",
    "        status += 'batch size: {}\\n'.format(self.batch_size)\n",
    "        status += 'embedding size: {}\\n'.format(self.embedding_size)\n",
    "        status += 'num words: {}\\n'.format(self.num_words)\n",
    "        status += 'lstm units: {}\\n'.format(self.lstm_units)\n",
    "        status += 'num classes: {}\\n'.format(self.num_classes)\n",
    "        status += 'epochs: {}\\n'.format(self.epochs)\n",
    "\n",
    "        return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "\n",
    "from model.model import Model\n",
    "\n",
    "\n",
    "class RecurrentModel(Model):\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        print(self.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_config = RecurrentConfig()\n",
    "tweet_model = RecurrentModel(tweet_config)\n",
    "\n",
    "tweet_model.build_model()\n",
    "\n",
    "train_data = (x_train, y_train)\n",
    "model_history = tweet_model.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate our test predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tweet_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the confusion matrix from our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils.utils import plot_confusion_matrix\n",
    "\n",
    "test_predictions = np.argmax(predictions, axis=1)\n",
    "test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "plot_confusion_matrix(test_labels, test_predictions, classes=[\"negative\", \"neutral\", \"positive\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model Is very good at classifying negative tweets, but is not doing so great on classifying neutral and positive tweets. Now, in that circunstances, there is a lot of things we could try to see if we get a better model:\n",
    "\n",
    "* Add regularization: We can add regularizers such as dropout or L2 to avoid model overffiting\n",
    "* We can reduce the number of units in the Recurrent Neural Net layer\n",
    "* We can balance the dataset before training the model\n",
    "* We can better filter our text data\n",
    "\n",
    "Try some of these approaches and see if you model has improvements. Also, remember to always plot the train vs validation graph to see how the model is behaving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
