{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Deep Learning Applications with Keras: Deploying your model using a REST API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trained a Deep Learning model and verifyed that is generalizing well, it is time to put our model into production. By production, this means that other applications or even real users will now be able to interact with this model in real time. Therefore, an user or an application will send data to our model and expect a prediction as a response. Most of the time, our model will be hosted in a server that will them receive a request and return a answer for this request. Therefore, to other applications or users to communicate with our model, we need to define its API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do this for an image classification model. Our model will receive an image and output which object it thinks that is present in that image. Therefore, we need to define 4 things for this model to handle requests:\n",
    "\n",
    "* Load the model in memory\n",
    "* Define how the model will receive a request containing an image\n",
    "* Pre-process the image to allow it to be used on the model\n",
    "* Define how the model prediction will be returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by first loading the model in memory. We can define the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "\n",
    "def load_model():\n",
    "    global model\n",
    "    model = ResNet50(weights=\"imagenet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before defining the model API, we know that it will receive an image and that our model only accepts images with a certain format to it. Let's define a function that receives an image and pre-process it for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications import imagenet_utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def pre_process_image(image, resize_size):\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    image = image.resize(resize_size)\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = imagenet_utils.preprocess_input(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this was text data, in this step we would convert our text into a list of ids, just as we seen on the LSTM notebook. This is very important step to remember. Every model that you implement and want to put it in production will receive unformarted data. Therefore, **always remember to pre-process this data in order to not crash the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have loaded the model in memory and defined a pre-processing function for the data it receives, we can now create an API. To create this API, we will use [Flask](http://flask.pocoo.org/). This python application will allow us to handle HTTP requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now initiate our Flask application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flask\n",
    "app = flask.Flask(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flask will also allow us to define an **end point** to our model.\n",
    "\n",
    "What do I mean by an **end point** ? We understand that our model will be running on a server. That server will have an address, let's say it is running on the address **0.0.0.0:8000**. Inside this address, we need to define how we are going to access our model. We need to identify our model as **resource** and give it an unique name. For example, we can define that our model will be found on the following name:\n",
    "\n",
    "**0.0.0.0:8000/model/**\n",
    "\n",
    "Now, our application will know that requests for this address will be sent to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we need to define which type of HTTP request our model will answer to. Since we will receive data to classify, we will need to define that the model will answer to [POST](https://en.wikipedia.org/wiki/POST_(HTTP) requests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to define how the model will send the prediction back to the user. This means that we must convert our prediction into a format that other applications will be able to parse. One of the most widely used formats for this task is the [json](https://www.json.org/json-en.html) format. This is a simple format that states that every data will be stored via a key-value pair. For example, suppose we want to inform for an user that the photo he has uploaded has a dog in it and the level of certainty our model has in that prediction. We can define our json response as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"prediction\": \"dog\",\n",
    "    \"certainty\": 0.98\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have pairs of variables in the json format, where the **key** identifies the variable and the **value**, obviously states the value of the key variable. It is very important for your application to define how it will send its reponse back to applications, so that these applications can act accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that said, let's define a function that handle **POST** request and sends a response back to the application that made the request. Our function will need to:\n",
    "\n",
    "* Extract the image from the request it has received\n",
    "* Pre-process the image\n",
    "* Use the model to make a prediction\n",
    "* Create a json file for that prediction and return that json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This decorator creates our end point. It states that our model will be found on the \"model\" address\n",
    "and that it will handle POST request.\n",
    "\"\"\"\n",
    "@app.route(\"/model\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    data = {\"success\": False}\n",
    "\n",
    "    if flask.request.method == \"POST\":\n",
    "            \n",
    "        \"\"\"\n",
    "        First let's extract the image from the request\n",
    "        \"\"\"\n",
    "        image = flask.request.files[\"image\"].read()\n",
    "        image = Image.open(io.BytesIO(image))\n",
    "\n",
    "        \"\"\"\n",
    "        Now, let's pre-process the image\n",
    "        \"\"\"\n",
    "        image = pre_process_image(image, resize_size=(224, 224))\n",
    "            \n",
    "        \"\"\"\n",
    "        Let's perform the classification over this image using our model\n",
    "        \"\"\"\n",
    "        preds = model.predict(image)\n",
    "        results = imagenet_utils.decode_predictions(preds)\n",
    "            \n",
    "        \"\"\"\n",
    "        Finally, let's create our json response and return it to the application that\n",
    "        sent the request.\n",
    "        \"\"\"\n",
    "            \n",
    "        data[\"predictions\"] = []\n",
    "\n",
    "        # loop over the results and add them to the list of\n",
    "        # returned predictions\n",
    "        for (imagenetID, label, prob) in results[0]:\n",
    "            r = {\"label\": label, \"probability\": float(prob)}\n",
    "            data[\"predictions\"].append(r)\n",
    "\n",
    "        data[\"success\"] = True\n",
    "\n",
    "    return flask.jsonify(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that function defined, let's test it. We will use the following image to test our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dog.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our application. We just need to run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "from werkzeug.serving import run_simple\n",
    "\n",
    "def run_server():\n",
    "    print(\"Loading model\")\n",
    "    load_model()\n",
    "    print(\"Running server\")\n",
    "    run_simple('0.0.0.0', 9000, app)\n",
    "\n",
    "p = Process(target=run_server)\n",
    "p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's send the dog image to our model and test if it is working properly. To do that, run the following **curl** command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X POST -F image=@images/dog.jpg 'http://localhost:9000/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that our API should return something similar to this json response:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "    \"predictions\":\n",
    "        [\n",
    "            {\"label\":\"beagle\",\"probability\":0.9901767373085022},\n",
    "            {\"label\":\"Walker_hound\",\"probability\":0.002248708624392748},\n",
    "            {\"label\":\"Brittany_spaniel\",\"probability\":0.0011901347897946835},\n",
    "            {\"label\":\"pot\",\"probability\":0.001180286519229412},\n",
    "            {\"label\":\"Cardigan\",\"probability\":0.0006831097998656332}\n",
    "        ],\n",
    "    \"success\":true\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see a similar response, then your model is running correctly.\n",
    "    \n",
    "If you are interested in model deployment, this is just the first step on that area. You can check more advanced ways to deploy your model, for example, how can I scale my model ? Should I use containers ?\n",
    "\n",
    "You can start learning about this topics reading this [post](https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/) and learning more about [docker](https://www.docker.com/). Model deployment is becoming more and more present when dealing with Deep Learning models and is always good to keep learning new techniques to handle that activity :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
